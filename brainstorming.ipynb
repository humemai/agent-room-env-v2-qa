{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple contextual bandit with replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Set up device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize parameters\n",
    "embedding_dim = 20  # Embedding dimension for entities and relations\n",
    "K = 50  # Total number of entities (arms)\n",
    "N_episodes = 1000  # Number of episodes\n",
    "M = 100  # Number of steps per episode\n",
    "batch_size = 32  # Batch size for replay buffer sampling\n",
    "buffer_size = 1000  # Size of the replay buffer\n",
    "noise_std = 0.1  # Standard deviation for the noise\n",
    "\n",
    "# Generate entity and relation embeddings (move to device)\n",
    "entity_embeddings = torch.tensor(\n",
    "    np.random.randn(K, embedding_dim), dtype=torch.float32, device=device\n",
    ")\n",
    "relation_embedding = torch.tensor(\n",
    "    np.random.randn(embedding_dim), dtype=torch.float32, device=device\n",
    ")  # Only one relation \"atLocation\"\n",
    "\n",
    "# True tail entities for each head entity\n",
    "# This represents the correct tail for each head entity (randomly generated for now)\n",
    "true_tail_entities = np.random.randint(0, K, size=K)\n",
    "\n",
    "\n",
    "# Define the MLP Model with more complexity\n",
    "class LinkPredictionMLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinkPredictionMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Increased neurons\n",
    "        self.fc2 = nn.Linear(64, 32)  # Another hidden layer for more complexity\n",
    "        self.fc3 = nn.Linear(32, 1)  # Output layer: predicts probability of reward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Output between 0 and 1 (probability)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the MLP model\n",
    "model = LinkPredictionMLP(input_size=embedding_dim * 3).to(\n",
    "    device\n",
    ")  # Concatenate [head, relation, tail] embeddings\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary rewards\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Replay buffer to store experiences\n",
    "replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "\n",
    "# Function to generate binary reward based on whether the predicted tail is correct\n",
    "def generate_reward(head_entity, predicted_tail):\n",
    "    true_tail = true_tail_entities[head_entity]  # Get the true tail entity for the head\n",
    "    return 1 if predicted_tail == true_tail else 0  # Reward is 1 if correct, else 0\n",
    "\n",
    "\n",
    "# Function to select an action (tail entity) based on MLP predictions (epsilon-greedy)\n",
    "def select_action(\n",
    "    model, head_embedding, relation_embedding, tail_entities, epsilon=0.1\n",
    "):\n",
    "    num_actions = tail_entities.shape[0]\n",
    "\n",
    "    # Permute the tail entities randomly\n",
    "    perm = torch.randperm(num_actions)\n",
    "    tail_entities_permuted = tail_entities[perm]\n",
    "\n",
    "    # Epsilon-greedy strategy: explore with probability epsilon, otherwise exploit\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, num_actions)  # Explore: choose a random entity\n",
    "    else:\n",
    "        # Exploit: choose the entity with the highest predicted reward\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.cat(\n",
    "                [\n",
    "                    head_embedding.repeat(num_actions, 1),\n",
    "                    relation_embedding.repeat(num_actions, 1),\n",
    "                    tail_entities_permuted,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            predictions = model(inputs)  # Predict reward probabilities for each entity\n",
    "\n",
    "        # Get the predicted index in the permuted order\n",
    "        predicted_index_permuted = torch.argmax(predictions).item()\n",
    "\n",
    "        # Restore the original index by inverting the permutation\n",
    "        predicted_index_original = perm[predicted_index_permuted].item()\n",
    "\n",
    "        return predicted_index_original\n",
    "\n",
    "\n",
    "# Function to train the MLP model using a batch of experiences from the replay buffer\n",
    "def train_model_batch(model, optimizer, replay_buffer):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return  # Don't train until the buffer is filled enough\n",
    "\n",
    "    # Sample a batch of experiences\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "    # Extract input vectors (concatenated [head, relation, tail]) and rewards\n",
    "    batch_inputs = torch.stack([experience[0] for experience in batch]).to(\n",
    "        device\n",
    "    )  # Contexts (head, relation, tail)\n",
    "    batch_rewards = torch.tensor(\n",
    "        [experience[1] for experience in batch], dtype=torch.float32, device=device\n",
    "    )  # Rewards\n",
    "\n",
    "    # Forward pass: predict rewards for each context in the batch\n",
    "    model.train()\n",
    "    predicted_rewards = model(batch_inputs)\n",
    "\n",
    "    # Compute loss between predicted rewards and actual rewards\n",
    "    loss = criterion(\n",
    "        predicted_rewards, batch_rewards.unsqueeze(1)\n",
    "    )  # Unsqueeze to match dimensions\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Initialize episode rewards storage\n",
    "episode_rewards = []\n",
    "\n",
    "# Epsilon parameters for decay\n",
    "epsilon = 1.0  # Start with high exploration\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "epsilon_decay = 0.999  # Slower decay to encourage more exploration\n",
    "\n",
    "# Simulate N episodes\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "for episode in tqdm(range(N_episodes)):\n",
    "    total_reward = 0  # Total reward for the episode\n",
    "\n",
    "    for step in range(M):\n",
    "        # Randomly select a head entity (e.g., Bob)\n",
    "        head_entity = np.random.randint(0, K)\n",
    "        head_embedding = entity_embeddings[head_entity].unsqueeze(\n",
    "            0\n",
    "        )  # Shape (1, embedding_dim)\n",
    "\n",
    "        # Add noise to head_embedding\n",
    "        head_embedding_noisy = head_embedding + noise_std * torch.randn_like(\n",
    "            head_embedding\n",
    "        )\n",
    "\n",
    "        # Randomly select a set of candidate tail entities (e.g., Kitchen, Bathroom, etc.)\n",
    "        tail_entity_indices = np.random.choice(\n",
    "            K, np.random.randint(1, K + 1), replace=False\n",
    "        )\n",
    "        tail_entity_embeddings = entity_embeddings[tail_entity_indices]\n",
    "\n",
    "        # Add noise to each tail entity embedding\n",
    "        tail_entity_embeddings_noisy = (\n",
    "            tail_entity_embeddings\n",
    "            + noise_std * torch.randn_like(tail_entity_embeddings)\n",
    "        )\n",
    "\n",
    "        # Select the best tail entity using epsilon-greedy strategy\n",
    "        chosen_tail_index = select_action(\n",
    "            model,\n",
    "            head_embedding_noisy,\n",
    "            relation_embedding.unsqueeze(0),\n",
    "            tail_entity_embeddings_noisy,\n",
    "            epsilon,\n",
    "        )\n",
    "        chosen_tail_entity = tail_entity_indices[chosen_tail_index]\n",
    "\n",
    "        # Generate binary reward (1 if correct, 0 if wrong)\n",
    "        reward = generate_reward(head_entity, chosen_tail_entity)\n",
    "\n",
    "        # Store the experience in the replay buffer (store the concatenation of [head, relation, tail])\n",
    "        input_vector = torch.cat(\n",
    "            [\n",
    "                head_embedding_noisy,\n",
    "                relation_embedding.unsqueeze(0),\n",
    "                entity_embeddings[chosen_tail_entity].unsqueeze(0),\n",
    "            ],\n",
    "            dim=1,\n",
    "        ).squeeze()\n",
    "        replay_buffer.append((input_vector, reward))\n",
    "\n",
    "        # Accumulate the reward for this episode\n",
    "        total_reward += reward\n",
    "\n",
    "        # Train the MLP model using a batch of experiences from the replay buffer\n",
    "        train_model_batch(model, optimizer, replay_buffer)\n",
    "\n",
    "    # Store the total reward for the current episode\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    # Decay epsilon after each episode\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "# Plot the episode rewards to see if the model is learning over time\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Total Reward per Episode over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple contextual bandit without replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set up device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize parameters\n",
    "embedding_dim = 20  # Embedding dimension for entities and relations\n",
    "K = 50  # Total number of entities (arms)\n",
    "N_episodes = 1000  # Number of episodes\n",
    "M = 100  # Number of steps per episode\n",
    "noise_std = 0.1  # Standard deviation for the noise\n",
    "\n",
    "# Generate entity and relation embeddings (move to device)\n",
    "entity_embeddings = torch.tensor(\n",
    "    np.random.randn(K, embedding_dim), dtype=torch.float32, device=device\n",
    ")\n",
    "relation_embedding = torch.tensor(\n",
    "    np.random.randn(embedding_dim), dtype=torch.float32, device=device\n",
    ")  # Only one relation \"atLocation\"\n",
    "\n",
    "# True tail entities for each head entity\n",
    "# This represents the correct tail for each head entity (randomly generated for now)\n",
    "true_tail_entities = np.random.randint(0, K, size=K)\n",
    "\n",
    "\n",
    "# Define the MLP Model with more complexity\n",
    "class LinkPredictionMLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinkPredictionMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Increased neurons\n",
    "        self.fc2 = nn.Linear(64, 32)  # Another hidden layer for more complexity\n",
    "        self.fc3 = nn.Linear(32, 1)  # Output layer: predicts probability of reward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Output between 0 and 1 (probability)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the MLP model\n",
    "model = LinkPredictionMLP(input_size=embedding_dim * 3).to(\n",
    "    device\n",
    ")  # Concatenate [head, relation, tail] embeddings\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary rewards\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Function to generate binary reward based on whether the predicted tail is correct\n",
    "def generate_reward(head_entity, predicted_tail):\n",
    "    true_tail = true_tail_entities[head_entity]  # Get the true tail entity for the head\n",
    "    return 1 if predicted_tail == true_tail else 0  # Reward is 1 if correct, else 0\n",
    "\n",
    "\n",
    "# Function to select an action (tail entity) based on MLP predictions (epsilon-greedy)\n",
    "def select_action(\n",
    "    model, head_embedding, relation_embedding, tail_entities, epsilon=0.1\n",
    "):\n",
    "    num_actions = tail_entities.shape[0]\n",
    "\n",
    "    # Permute the tail entities randomly\n",
    "    perm = torch.randperm(num_actions)\n",
    "    tail_entities_permuted = tail_entities[perm]\n",
    "\n",
    "    # Epsilon-greedy strategy: explore with probability epsilon, otherwise exploit\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, num_actions)  # Explore: choose a random entity\n",
    "    else:\n",
    "        # Exploit: choose the entity with the highest predicted reward\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.cat(\n",
    "                [\n",
    "                    head_embedding.repeat(num_actions, 1),\n",
    "                    relation_embedding.repeat(num_actions, 1),\n",
    "                    tail_entities_permuted,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            predictions = model(inputs)  # Predict reward probabilities for each entity\n",
    "\n",
    "        # Get the predicted index in the permuted order\n",
    "        predicted_index_permuted = torch.argmax(predictions).item()\n",
    "\n",
    "        # Restore the original index by inverting the permutation\n",
    "        predicted_index_original = perm[predicted_index_permuted].item()\n",
    "\n",
    "        return predicted_index_original\n",
    "\n",
    "\n",
    "# Function to train the MLP model online (update after each experience)\n",
    "def train_model_online(model, optimizer, input_vector, reward):\n",
    "    # Forward pass: predict reward for the current context\n",
    "    model.train()\n",
    "    predicted_reward = model(\n",
    "        input_vector.unsqueeze(0)\n",
    "    )  # Add batch dimension (1, input_size)\n",
    "\n",
    "    # Compute loss between predicted reward and actual reward\n",
    "    reward_tensor = torch.tensor([[reward]], dtype=torch.float32, device=device)\n",
    "    loss = criterion(predicted_reward, reward_tensor)\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Initialize episode rewards storage\n",
    "episode_rewards = []\n",
    "\n",
    "# Epsilon parameters for decay\n",
    "epsilon = 1.0  # Start with high exploration\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "epsilon_decay = 0.999  # Slower decay to encourage more exploration\n",
    "\n",
    "# Simulate N episodes\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "for episode in tqdm(range(N_episodes)):\n",
    "    total_reward = 0  # Total reward for the episode\n",
    "\n",
    "    for step in range(M):\n",
    "        # Randomly select a head entity (e.g., Bob)\n",
    "        head_entity = np.random.randint(0, K)\n",
    "        head_embedding = entity_embeddings[head_entity].unsqueeze(\n",
    "            0\n",
    "        )  # Shape (1, embedding_dim)\n",
    "\n",
    "        # Add noise to head_embedding\n",
    "        head_embedding_noisy = head_embedding + noise_std * torch.randn_like(\n",
    "            head_embedding\n",
    "        )\n",
    "\n",
    "        # Randomly select a set of candidate tail entities (e.g., Kitchen, Bathroom, etc.)\n",
    "        tail_entity_indices = np.random.choice(\n",
    "            K, np.random.randint(1, K + 1), replace=False\n",
    "        )\n",
    "        tail_entity_embeddings = entity_embeddings[tail_entity_indices]\n",
    "\n",
    "        # Add noise to each tail entity embedding\n",
    "        tail_entity_embeddings_noisy = (\n",
    "            tail_entity_embeddings\n",
    "            + noise_std * torch.randn_like(tail_entity_embeddings)\n",
    "        )\n",
    "\n",
    "        # Select the best tail entity using epsilon-greedy strategy\n",
    "        chosen_tail_index = select_action(\n",
    "            model,\n",
    "            head_embedding_noisy,\n",
    "            relation_embedding.unsqueeze(0),\n",
    "            tail_entity_embeddings_noisy,\n",
    "            epsilon,\n",
    "        )\n",
    "        chosen_tail_entity = tail_entity_indices[chosen_tail_index]\n",
    "\n",
    "        # Generate binary reward (1 if correct, 0 if wrong)\n",
    "        reward = generate_reward(head_entity, chosen_tail_entity)\n",
    "\n",
    "        # Prepare input vector for the model (concatenation of [head, relation, tail])\n",
    "        input_vector = torch.cat(\n",
    "            [\n",
    "                head_embedding_noisy,\n",
    "                relation_embedding.unsqueeze(0),\n",
    "                entity_embeddings[chosen_tail_entity].unsqueeze(0),\n",
    "            ],\n",
    "            dim=1,\n",
    "        ).squeeze()\n",
    "\n",
    "        # Online training: update the model with this single experience\n",
    "        train_model_online(model, optimizer, input_vector, reward)\n",
    "\n",
    "        # Accumulate the reward for this episode\n",
    "        total_reward += reward\n",
    "\n",
    "    # Store the total reward for the current episode\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    # Decay epsilon after each episode\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "# Plot the episode rewards to see if the model is learning over time\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Total Reward per Episode Over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "foo = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "reshape(input, shape) -> Tensor\n",
      "\n",
      "Returns a tensor with the same data and number of elements as :attr:`input`,\n",
      "but with the specified shape. When possible, the returned tensor will be a view\n",
      "of :attr:`input`. Otherwise, it will be a copy. Contiguous inputs and inputs\n",
      "with compatible strides can be reshaped without copying, but you should not\n",
      "depend on the copying vs. viewing behavior.\n",
      "\n",
      "See :meth:`torch.Tensor.view` on when it is possible to return a view.\n",
      "\n",
      "A single dimension may be -1, in which case it's inferred from the remaining\n",
      "dimensions and the number of elements in :attr:`input`.\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the tensor to be reshaped\n",
      "    shape (tuple of int): the new shape\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> a = torch.arange(4.)\n",
      "    >>> torch.reshape(a, (2, 2))\n",
      "    tensor([[ 0.,  1.],\n",
      "            [ 2.,  3.]])\n",
      "    >>> b = torch.tensor([[0, 1], [2, 3]])\n",
      "    >>> torch.reshape(b, (-1,))\n",
      "    tensor([ 0,  1,  2,  3])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "?torch.reshape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-room-env-v2-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
