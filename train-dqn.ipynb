{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MM / explore with random sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from agent import DQNAgent\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Number of combinations you want\n",
    "num_combinations = 500  # Change this to however many combinations you need\n",
    "\n",
    "# default\n",
    "room_size = \"xl-different-prob\"\n",
    "capacity_max = 6\n",
    "terminates_at = 99\n",
    "num_iterations = (terminates_at + 1) * 100\n",
    "\n",
    "prob_type = (\n",
    "    \"non-equal-object-probs\" if \"different-prob\" in room_size else \"equal-object-probs\"\n",
    ")\n",
    "root_path = (\n",
    "    f\"./training-results/{prob_type}/dqn/room_size={room_size}/capacity={capacity_max}/\"\n",
    ")\n",
    "\n",
    "# root_path = f\"training-results/TRASH/{room_size}\"\n",
    "\n",
    "# random\n",
    "replay_buffer_size_ = [num_iterations // 10]\n",
    "test_seed_ = [i for i in range(num_combinations)]\n",
    "target_update_interval_ = [50, 100]\n",
    "gamma_ = [0.99, 0.9]\n",
    "semantic_decay_factor_ = [0.8]\n",
    "pretrain_semantic_ = [False]\n",
    "relu_between_gcn_layers_ = [False, True]\n",
    "dropout_between_gcn_layers_ = [False, True]\n",
    "num_layers_ = [2, 4]\n",
    "batch_size_ = [32, 64]\n",
    "embedding_dim_ = [32, 64]\n",
    "triple_qual_weight_ = [0.8]\n",
    "intrinsic_explore_reward_ = [0.5, 1.0, 2.0, 5.0, 10]\n",
    "learning_rate_ = [0.001, 0.0001]\n",
    "explore_policy_ = [\"rl\"]\n",
    "mm_policy_ = [\"rl\"]\n",
    "scale_reward_ = [True, False]\n",
    "gcn_type_ = [\"stare\"]\n",
    "\n",
    "\n",
    "# Generate all combinations\n",
    "params_all = list(\n",
    "    itertools.product(\n",
    "        test_seed_,\n",
    "        target_update_interval_,\n",
    "        gamma_,\n",
    "        semantic_decay_factor_,\n",
    "        pretrain_semantic_,\n",
    "        replay_buffer_size_,\n",
    "        relu_between_gcn_layers_,\n",
    "        dropout_between_gcn_layers_,\n",
    "        num_layers_,\n",
    "        batch_size_,\n",
    "        embedding_dim_,\n",
    "        triple_qual_weight_,\n",
    "        intrinsic_explore_reward_,\n",
    "        learning_rate_,\n",
    "        explore_policy_,\n",
    "        mm_policy_,\n",
    "        scale_reward_,\n",
    "        gcn_type_,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Random combinations with weighted agent_capacity_\n",
    "random_combinations = random.sample(params_all, num_combinations)\n",
    "\n",
    "for i, params in tqdm(enumerate(random_combinations)):\n",
    "    (\n",
    "        test_seed,\n",
    "        target_update_interval,\n",
    "        gamma,\n",
    "        semantic_decay_factor,\n",
    "        pretrain_semantic,\n",
    "        replay_buffer_size,\n",
    "        relu_between_gcn_layers,\n",
    "        dropout_between_gcn_layers,\n",
    "        num_layers,\n",
    "        batch_size,\n",
    "        embedding_dim,\n",
    "        triple_qual_weight,\n",
    "        intrinsic_explore_reward,\n",
    "        learning_rate,\n",
    "        explore_policy,\n",
    "        mm_policy,\n",
    "        scale_reward,\n",
    "        gcn_type,\n",
    "    ) = params\n",
    "\n",
    "    params_dict = {\n",
    "        \"env_str\": \"room_env:RoomEnv-v2\",\n",
    "        \"num_iterations\": num_iterations,\n",
    "        \"replay_buffer_size\": replay_buffer_size,\n",
    "        \"warm_start\": batch_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"target_update_interval\": target_update_interval,\n",
    "        \"epsilon_decay_until\": num_iterations,\n",
    "        \"max_epsilon\": 1.0,\n",
    "        \"min_epsilon\": 0.1,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"capacity\": {\"long\": capacity_max, \"short\": 15},\n",
    "        \"pretrain_semantic\": pretrain_semantic,\n",
    "        \"semantic_decay_factor\": semantic_decay_factor,\n",
    "        \"dqn_params\": {\n",
    "            \"gcn_layer_params\": {\n",
    "                \"type\": gcn_type,\n",
    "                \"embedding_dim\": embedding_dim,\n",
    "                \"num_layers\": num_layers,\n",
    "                \"gcn_drop\": 0.1,\n",
    "                \"triple_qual_weight\": triple_qual_weight,\n",
    "            },\n",
    "            \"relu_between_gcn_layers\": relu_between_gcn_layers,\n",
    "            \"dropout_between_gcn_layers\": dropout_between_gcn_layers,\n",
    "            \"mlp_params\": {\"num_hidden_layers\": num_layers, \"dueling_dqn\": True},\n",
    "        },\n",
    "        \"num_samples_for_results\": {\"val\": 5, \"test\": 10},\n",
    "        \"validation_interval\": 1,\n",
    "        \"plotting_interval\": 50,\n",
    "        \"train_seed\": test_seed + 5,\n",
    "        \"test_seed\": test_seed,\n",
    "        \"device\": \"cpu\",\n",
    "        \"qa_function\": \"latest_strongest\",\n",
    "        \"env_config\": {\n",
    "            \"question_prob\": 1.0,\n",
    "            \"terminates_at\": terminates_at,\n",
    "            \"randomize_observations\": \"all\",\n",
    "            \"room_size\": room_size,\n",
    "            \"rewards\": {\"correct\": 1, \"wrong\": 0, \"partial\": 0},\n",
    "            \"make_everything_static\": False,\n",
    "            \"num_total_questions\": 1000,\n",
    "            \"question_interval\": 1,\n",
    "            \"include_walls_in_observations\": True,\n",
    "        },\n",
    "        \"intrinsic_explore_reward\": intrinsic_explore_reward,\n",
    "        \"ddqn\": True,\n",
    "        \"default_root_dir\": root_path,\n",
    "        \"explore_policy\": explore_policy,\n",
    "        \"mm_policy\": mm_policy,\n",
    "        \"scale_reward\": scale_reward,\n",
    "    }\n",
    "\n",
    "    agent = DQNAgent(**params_dict)\n",
    "    agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fixed combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "import os\n",
    "from agent import DQNAgent\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "room_size = \"xxl-different-prob\"\n",
    "terminates_at = 99\n",
    "num_iterations = (terminates_at + 1) * 200\n",
    "batch_size = 32\n",
    "semantic_decay_factor = 0.8\n",
    "num_layers = 2\n",
    "triple_qual_weight = 0.8\n",
    "\n",
    "for test_seed in [5, 6, 7, 8]:\n",
    "    for capacity_max in [96]:\n",
    "        for replay_buffer_size in [num_iterations]:\n",
    "            for gamma in [{\"mm\": 0.95, \"explore\": 0.95}]:\n",
    "                for target_update_interval in [100]:\n",
    "                    for embedding_dim in [90]:\n",
    "                        prob_type = (\n",
    "                            \"non-equal-object-probs\"\n",
    "                            if \"different-prob\" in room_size\n",
    "                            else \"equal-object-probs\"\n",
    "                        )\n",
    "                        root_path = (\n",
    "                            f\"./training-results/{prob_type}/dqn/\"\n",
    "                            f\"room_size={room_size}/capacity={capacity_max}/\"\n",
    "                        )\n",
    "                        for pretrain_semantic in [False]:\n",
    "                            params_dict = {\n",
    "                                \"env_str\": \"room_env:RoomEnv-v2\",\n",
    "                                \"num_iterations\": num_iterations,\n",
    "                                \"replay_buffer_size\": replay_buffer_size,\n",
    "                                \"warm_start\": batch_size,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"target_update_interval\": target_update_interval,\n",
    "                                \"epsilon_decay_until\": num_iterations,\n",
    "                                \"max_epsilon\": 1.0,\n",
    "                                \"min_epsilon\": 0.1,\n",
    "                                \"gamma\": gamma,\n",
    "                                \"learning_rate\": 0.001,\n",
    "                                \"capacity\": {\"long\": capacity_max, \"short\": 15},\n",
    "                                \"pretrain_semantic\": pretrain_semantic,\n",
    "                                \"semantic_decay_factor\": semantic_decay_factor,\n",
    "                                \"dqn_params\": {\n",
    "                                    \"gcn_layer_params\": {\n",
    "                                        \"type\": \"stare\",\n",
    "                                        \"embedding_dim\": embedding_dim,\n",
    "                                        \"num_layers\": num_layers,\n",
    "                                        \"gcn_drop\": 0.1,\n",
    "                                        \"triple_qual_weight\": triple_qual_weight,\n",
    "                                    },\n",
    "                                    \"relu_between_gcn_layers\": True,\n",
    "                                    \"dropout_between_gcn_layers\": False,\n",
    "                                    \"mlp_params\": {\n",
    "                                        \"num_hidden_layers\": num_layers,\n",
    "                                        \"dueling_dqn\": True,\n",
    "                                    },\n",
    "                                },\n",
    "                                \"num_samples_for_results\": {\"val\": 5, \"test\": 10},\n",
    "                                \"validation_interval\": 1,\n",
    "                                \"plotting_interval\": 50,\n",
    "                                \"train_seed\": test_seed + 5,\n",
    "                                \"test_seed\": test_seed,\n",
    "                                \"device\": \"cpu\",\n",
    "                                \"qa_function\": \"latest_strongest\",\n",
    "                                \"env_config\": {\n",
    "                                    \"question_prob\": 1.0,\n",
    "                                    \"terminates_at\": terminates_at,\n",
    "                                    \"randomize_observations\": \"all\",\n",
    "                                    \"room_size\": room_size,\n",
    "                                    \"rewards\": {\"correct\": 1, \"wrong\": 0, \"partial\": 0},\n",
    "                                    \"make_everything_static\": False,\n",
    "                                    \"num_total_questions\": 1000,\n",
    "                                    \"question_interval\": 1,\n",
    "                                    \"include_walls_in_observations\": True,\n",
    "                                },\n",
    "                                \"intrinsic_explore_reward\": 0,\n",
    "                                \"ddqn\": True,\n",
    "                                \"default_root_dir\": root_path,\n",
    "                                \"explore_policy\": \"rl\",\n",
    "                                \"mm_policy\": \"rl\",\n",
    "                                \"scale_reward\": False,\n",
    "                            }\n",
    "\n",
    "                            agent = DQNAgent(**params_dict)\n",
    "                            agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.virtualenvs/agent-room-env-v2-qa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tk/.virtualenvs/agent-room-env-v2-qa/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:168: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/tk/.virtualenvs/agent-room-env-v2-qa/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/tk/.virtualenvs/agent-room-env-v2-qa/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:127: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method should be an int or np.int64, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "/home/tk/.virtualenvs/agent-room-env-v2-qa/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "Assertion passed: dict A is part of dict B.\n",
      "> \u001b[0;32m/home/tk/repos/agent-room-env-v2-qa/agent/dqn/nn/gnn.py\u001b[0m(513)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    511 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    512 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 513 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mlayer_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    514 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0;34m\"stare\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    515 \u001b[0;31m                entity_embeddings, relation_embeddings = layer_(\n",
      "\u001b[0m\n",
      "[['west_inv', 'west', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation']]\n",
      "[['west_inv', 'west', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation']]\n",
      "[['west_inv', 'west', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation'], ['west_inv', 'west', 'timestamp', 'strength', 'south_inv', 'south', 'north_inv', 'north', 'east_inv', 'east', 'current_time', 'atlocation_inv', 'atlocation']]\n",
      "[['wall', 'room_004', 'room_001', 'room_000', 'dep_007', 'dep_001', 'agent', '0'], ['wall', 'room_005', 'room_001', 'room_000', 'dep_007', 'agent', '1', '0'], ['wall', 'room_006', 'room_005', 'room_004', 'room_001', 'room_000', 'dep_007', 'agent', '2', '1', '0'], ['wall', 'sta_004', 'room_010', 'room_007', 'room_006', 'room_005', 'room_001', 'room_000', 'dep_007', 'agent', '3', '2', '1', '0']]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from agent.dqn import DQNAgent\n",
    "\n",
    "room_size = \"xl-different-prob\"\n",
    "terminates_at = 99\n",
    "num_iterations = (terminates_at + 1) * 1\n",
    "replay_buffer_size = 16\n",
    "batch_size = 4\n",
    "semantic_decay_factor = 0.8\n",
    "num_layers = 2\n",
    "triple_qual_weight = 0.8\n",
    "embedding_dim = 64\n",
    "target_update_interval = 10\n",
    "capacity_max = 12\n",
    "test_seed = 0\n",
    "pretrain_semantic = False\n",
    "\n",
    "prob_type = (\n",
    "    \"non-equal-object-probs\"\n",
    "    if \"different-prob\" in room_size\n",
    "    else \"equal-object-probs\"\n",
    ")\n",
    "root_path = (\n",
    "    f\"./training-results/{prob_type}/dqn/\"\n",
    "    f\"room_size={room_size}/capacity={capacity_max}/\"\n",
    ")\n",
    "if capacity_max == 192:\n",
    "    pretrained_path = \"trained-results/non-equal-object-probs/dqn/room_size=xl-different-prob/capacity=192/2024-08-12 12:58:16.107541/\"\n",
    "elif capacity_max == 96:\n",
    "    pretrained_path = \"trained-results/non-equal-object-probs/dqn/room_size=xl-different-prob/capacity=96/2024-08-12 23:58:06.290168/\"\n",
    "elif capacity_max == 48:\n",
    "    pretrained_path = \"trained-results/non-equal-object-probs/dqn/room_size=xl-different-prob/capacity=48/2024-08-11 11:07:00.648864/\"\n",
    "elif capacity_max == 24:\n",
    "    pretrained_path = \"trained-results/non-equal-object-probs/dqn/room_size=xl-different-prob/capacity=24/2024-08-11 13:36:54.499426/\"\n",
    "elif capacity_max == 12:\n",
    "    pretrained_path = \"trained-results/non-equal-object-probs/dqn/room_size=xl-different-prob/capacity=12/2024-08-11 16:24:54.492650/\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid capacity_max: {capacity_max}\")\n",
    "\n",
    "params_dict = {\n",
    "    \"env_str\": \"room_env:RoomEnv-v2\",\n",
    "    \"num_iterations\": num_iterations,\n",
    "    \"replay_buffer_size\": replay_buffer_size,\n",
    "    \"warm_start\": batch_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"target_update_interval\": target_update_interval,\n",
    "    \"epsilon_decay_until\": num_iterations,\n",
    "    \"max_epsilon\": 1.0,\n",
    "    \"min_epsilon\": 0.1,\n",
    "    \"gamma\": {\"mm\": 0.90, \"explore\": 0.90},\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"capacity\": {\"long\": capacity_max, \"short\": 15},\n",
    "    \"pretrain_semantic\": pretrain_semantic,\n",
    "    \"semantic_decay_factor\": semantic_decay_factor,\n",
    "    \"dqn_params\": {\n",
    "        \"gcn_layer_params\": {\n",
    "            \"type\": \"stare\",\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"gcn_drop\": 0.1,\n",
    "            \"triple_qual_weight\": triple_qual_weight,\n",
    "        },\n",
    "        \"relu_between_gcn_layers\": True,\n",
    "        \"dropout_between_gcn_layers\": False,\n",
    "        \"mlp_params\": {\n",
    "            \"num_hidden_layers\": num_layers,\n",
    "            \"dueling_dqn\": True,\n",
    "        },\n",
    "    },\n",
    "    \"num_samples_for_results\": {\"val\": 1, \"test\": 10},\n",
    "    \"validation_interval\": 1,\n",
    "    \"plotting_interval\": 50,\n",
    "    \"train_seed\": test_seed + 5,\n",
    "    \"test_seed\": test_seed,\n",
    "    \"device\": \"cpu\",\n",
    "    \"env_config\": {\n",
    "        \"question_prob\": 1.0,\n",
    "        \"terminates_at\": terminates_at,\n",
    "        \"randomize_observations\": \"all\",\n",
    "        \"room_size\": room_size,\n",
    "        \"rewards\": {\"correct\": 1, \"wrong\": 0, \"partial\": 0},\n",
    "        \"make_everything_static\": False,\n",
    "        \"num_total_questions\": 1000,\n",
    "        \"question_interval\": 1,\n",
    "        \"include_walls_in_observations\": True,\n",
    "    },\n",
    "    \"intrinsic_explore_reward\": 0,\n",
    "    \"ddqn\": True,\n",
    "    \"default_root_dir\": root_path,\n",
    "    \"explore_policy\": \"neural\",\n",
    "    \"mm_policy\": \"neural\",\n",
    "    \"qa_function\": \"bandit\",\n",
    "    \"pretrained_path\": pretrained_path,\n",
    "    \"llm_params\": {\n",
    "        \"model_id\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"quantization\": \"4bit\",\n",
    "        \"max_new_tokens\": 32,\n",
    "    },\n",
    "    \"scale_reward\": False,\n",
    "}\n",
    "\n",
    "agent = DQNAgent(**params_dict)\n",
    "agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.randn(10,12)[:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor([ 0.1815,  0.0025,  0.0979, -0.0953, -0.0693,  0.0169,  0.1384, -0.0427,\n",
    "         0.1296,  0.0261, -0.1082, -0.0142, -0.1870, -0.0642, -0.0292, -0.1634,\n",
    "        -0.2894,  0.0402, -0.0284,  0.1223,  0.0674,  0.0541,  0.0152, -0.1415,\n",
    "         0.1151, -0.1326, -0.1931,  0.0393,  0.0150, -0.0898,  0.0099,  0.0076,\n",
    "         0.0371, -0.0550, -0.1216,  0.1665,  0.1726,  0.0524, -0.0776,  0.0290,\n",
    "         0.1052,  0.0510,  0.0469, -0.0721, -0.0614, -0.0033,  0.0174, -0.0381,\n",
    "         0.0197, -0.1537,  0.0900, -0.1006, -0.0351,  0.0049, -0.0163, -0.1509,\n",
    "        -0.1135, -0.1118, -0.0411,  0.0645, -0.0284, -0.0641, -0.0260, -0.1871]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.randn(10), torch.randn(10)], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[\n",
    "    [\"agent\", \"atlocation\", \"room_000\", {\"current_time\": 0}],\n",
    "    [\"room_000\", \"east\", \"room_001\", {\"current_time\": 0}],\n",
    "    [\"dep_001\", \"atlocation\", \"room_000\", {\"current_time\": 0}],\n",
    "    [\"room_000\", \"west\", \"wall\", {\"current_time\": 0}],\n",
    "    [\"dep_007\", \"atlocation\", \"room_000\", {\"current_time\": 0}],\n",
    "    [\"room_000\", \"north\", \"wall\", {\"current_time\": 0}],\n",
    "    [\"room_000\", \"south\", \"room_004\", {\"current_time\": 0}],\n",
    "]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(\n",
    "    [\n",
    "        list(\n",
    "            [\n",
    "                [\"agent\", \"atlocation\", \"room_000\", {\"current_time\": 0}],\n",
    "                [\"room_000\", \"east\", \"room_001\", {\"current_time\": 0}],\n",
    "                [\"dep_001\", \"atlocation\", \"room_000\", {\"current_time\": 0}],\n",
    "                [\"room_000\", \"west\", \"wall\", {\"current_time\": 0}],\n",
    "                [\"dep_007\", \"atlocation\", \"room_000\", {\"current_time\": 0}],\n",
    "                [\"room_000\", \"north\", \"wall\", {\"current_time\": 0}],\n",
    "                [\"room_000\", \"south\", \"room_004\", {\"current_time\": 0}],\n",
    "            ]\n",
    "        ),\n",
    "        list(\n",
    "            [\n",
    "                [\"room_001\", \"south\", \"room_005\", {\"current_time\": 1}],\n",
    "                [\"agent\", \"atlocation\", \"room_001\", {\"current_time\": 1}],\n",
    "                [\"room_001\", \"west\", \"room_000\", {\"current_time\": 1}],\n",
    "                [\"room_001\", \"north\", \"wall\", {\"current_time\": 1}],\n",
    "                [\"room_001\", \"east\", \"wall\", {\"current_time\": 1}],\n",
    "                [\"dep_007\", \"atlocation\", \"room_000\", {\"timestamp\": [0]}],\n",
    "                [\"room_000\", \"north\", \"wall\", {\"strength\": 1}],\n",
    "            ]\n",
    "        ),\n",
    "        list(\n",
    "            [\n",
    "                [\"room_005\", \"east\", \"room_006\", {\"current_time\": 2}],\n",
    "                [\"agent\", \"atlocation\", \"room_005\", {\"current_time\": 2}],\n",
    "                [\"room_005\", \"north\", \"room_001\", {\"current_time\": 2}],\n",
    "                [\"room_005\", \"south\", \"wall\", {\"current_time\": 2}],\n",
    "                [\"room_005\", \"west\", \"room_004\", {\"current_time\": 2}],\n",
    "                [\"dep_007\", \"atlocation\", \"room_000\", {\"timestamp\": [0]}],\n",
    "                [\"room_000\", \"north\", \"wall\", {\"strength\": 1}],\n",
    "                [\"agent\", \"atlocation\", \"room_001\", {\"strength\": 1}],\n",
    "                [\"room_001\", \"west\", \"room_000\", {\"timestamp\": [1]}],\n",
    "                [\"room_001\", \"north\", \"wall\", {\"strength\": 1}],\n",
    "                [\"room_001\", \"east\", \"wall\", {\"timestamp\": [1]}],\n",
    "            ]\n",
    "        ),\n",
    "        list(\n",
    "            [\n",
    "                [\"agent\", \"atlocation\", \"room_006\", {\"current_time\": 3}],\n",
    "                [\"room_006\", \"north\", \"wall\", {\"current_time\": 3}],\n",
    "                [\"sta_004\", \"atlocation\", \"room_006\", {\"current_time\": 3}],\n",
    "                [\"room_006\", \"south\", \"room_010\", {\"current_time\": 3}],\n",
    "                [\"room_006\", \"west\", \"room_005\", {\"current_time\": 3}],\n",
    "                [\"room_006\", \"east\", \"room_007\", {\"current_time\": 3}],\n",
    "                [\"dep_007\", \"atlocation\", \"room_000\", {\"timestamp\": [0]}],\n",
    "                [\"room_000\", \"north\", \"wall\", {\"strength\": 1}],\n",
    "                [\"agent\", \"atlocation\", \"room_001\", {\"strength\": 1}],\n",
    "                [\"room_001\", \"west\", \"room_000\", {\"timestamp\": [1]}],\n",
    "                [\"room_001\", \"north\", \"wall\", {\"strength\": 1}],\n",
    "                [\"room_001\", \"east\", \"wall\", {\"timestamp\": [1]}],\n",
    "                [\"room_005\", \"east\", \"room_006\", {\"timestamp\": [2]}],\n",
    "                [\"room_005\", \"north\", \"room_001\", {\"strength\": 1}],\n",
    "            ]\n",
    "        ),\n",
    "    ],\n",
    "    dtype=object,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    [\"sta_006\", \"atlocation\", \"?\", 0],\n",
    "    [\"ind_001\", \"atlocation\", \"?\", 0],\n",
    "    [\"sta_000\", \"atlocation\", \"?\", 0],\n",
    "    [\"sta_002\", \"atlocation\", \"?\", 0],\n",
    "    [\"sta_003\", \"atlocation\", \"?\", 0],\n",
    "    [\"dep_005\", \"atlocation\", \"?\", 0],\n",
    "    [\"sta_004\", \"atlocation\", \"?\", 0],\n",
    "    [\"dep_002\", \"atlocation\", \"?\", 0],\n",
    "    [\"dep_005\", \"atlocation\", \"?\", 0],\n",
    "    [\"dep_005\", \"atlocation\", \"?\", 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<agent.dqn.dqn.DQNAgent at 0x72006a157730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'static': ['sta_000',\n",
       "  'sta_001',\n",
       "  'sta_002',\n",
       "  'sta_003',\n",
       "  'sta_004',\n",
       "  'sta_005',\n",
       "  'sta_006',\n",
       "  'sta_007'],\n",
       " 'independent': ['ind_000',\n",
       "  'ind_001',\n",
       "  'ind_002',\n",
       "  'ind_003',\n",
       "  'ind_004',\n",
       "  'ind_005',\n",
       "  'ind_006',\n",
       "  'ind_007'],\n",
       " 'dependent': ['dep_000',\n",
       "  'dep_001',\n",
       "  'dep_002',\n",
       "  'dep_003',\n",
       "  'dep_004',\n",
       "  'dep_005',\n",
       "  'dep_006',\n",
       "  'dep_007'],\n",
       " 'agent': ['agent'],\n",
       " 'room': ['room_000',\n",
       "  'room_001',\n",
       "  'room_002',\n",
       "  'room_003',\n",
       "  'room_004',\n",
       "  'room_005',\n",
       "  'room_006',\n",
       "  'room_007',\n",
       "  'room_008',\n",
       "  'room_009',\n",
       "  'room_010',\n",
       "  'room_011',\n",
       "  'room_012',\n",
       "  'room_013',\n",
       "  'room_014',\n",
       "  'room_015',\n",
       "  'room_016',\n",
       "  'room_017',\n",
       "  'room_018',\n",
       "  'room_019',\n",
       "  'room_020',\n",
       "  'room_021',\n",
       "  'room_022',\n",
       "  'room_023',\n",
       "  'room_024',\n",
       "  'room_025',\n",
       "  'room_026',\n",
       "  'room_027',\n",
       "  'room_028',\n",
       "  'room_029',\n",
       "  'room_030',\n",
       "  'room_031'],\n",
       " 'others': ['wall']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.unwrapped.entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[for foo in range(10) for bar in range(10)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-memory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
